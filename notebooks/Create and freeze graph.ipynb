{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the datasets into memory\n",
    "There are better ways to do this, but since the datasets are small enough, we can keep it simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MEAN_PIXEL_VALUE = 128\n",
    "PIXEL_STANDARD_DEVIATION = 80\n",
    "SEGMENT_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Converts the given image to grayscale, subtracts the mean*, and divides by the standard deviation*.\n",
    "    \n",
    "    * (roughly)\n",
    "    \n",
    "    The returned np.array uses 16-bit floats to conserve memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    global MEAN_PIXEL_VALUE\n",
    "    global PIXEL_STANDARD_DEVIATION\n",
    "    \n",
    "    image = image.convert('L')\n",
    "    image = np.array(image).astype(np.float16)\n",
    "    image = image - MEAN_PIXEL_VALUE\n",
    "    image = image / PIXEL_STANDARD_DEVIATION\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os import walk\n",
    "from PIL import Image\n",
    "\n",
    "def load_images(dir_path, n_max=-1):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    \n",
    "    for filename in next(walk(dir_path))[2]:\n",
    "        if len(images) == n_max:\n",
    "            break\n",
    "        \n",
    "        images.append(preprocess_image(Image.open(dir_path + filename)))\n",
    "        \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_images = []\n",
    "training_labels = []\n",
    "\n",
    "training_images.extend(load_images('../data/train/no-text/'))\n",
    "n_no_text = len(training_images)\n",
    "training_labels.extend([[1, 0] for i in range(n_no_text)])\n",
    "\n",
    "training_images.extend(load_images('../data/train/text/'))\n",
    "n_text = len(training_images) - n_no_text\n",
    "training_labels.extend([[0, 1] for i in range(n_text)])\n",
    "\n",
    "training_images = np.array(training_images)\n",
    "training_labels = np.array(training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "validation_images = []\n",
    "validation_labels = []\n",
    "\n",
    "validation_images.extend(load_images('../data/valid/no-text/'))\n",
    "n_no_text = len(validation_images)\n",
    "validation_labels.extend([[1, 0] for i in range(n_no_text)])\n",
    "\n",
    "validation_images.extend(load_images('../data/valid/text/'))\n",
    "n_text = len(validation_images) - n_no_text\n",
    "validation_labels.extend([[0, 1] for i in range(n_text)])\n",
    "\n",
    "validation_images = np.array(validation_images)\n",
    "validation_labels = np.array(validation_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(batch_size, use_training_set=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    if use_training_set:\n",
    "        indexes = np.random.choice(np.arange(len(training_images)), batch_size, False)\n",
    "        images = training_images[indexes]\n",
    "        labels = training_labels[indexes]\n",
    "    else:\n",
    "        indexes = np.random.choice(np.arange(len(validation_images)), batch_size, False)\n",
    "        images = validation_images[indexes]\n",
    "        labels = validation_labels[indexes]\n",
    "        \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def fill_feed_dict(pl_images, pl_labels, pl_learning_rate, pl_keep_prob, batch_size, learning_rate, keep_prob):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    global SEGMENT_SIZE\n",
    "    \n",
    "    training = learning_rate != 0\n",
    "    images, labels = get_batch(batch_size, training)\n",
    "    \n",
    "    images = np.reshape(images, (batch_size, SEGMENT_SIZE, SEGMENT_SIZE, 1))\n",
    "    labels = np.reshape(labels, (batch_size, 2))\n",
    "    \n",
    "    feed_dict = {\n",
    "        pl_images: images,\n",
    "        pl_labels: labels,\n",
    "        pl_learning_rate: learning_rate,\n",
    "        #pl_keep_prob: keep_prob # Disabled so that the graph can run on Android\n",
    "    }\n",
    "    \n",
    "    return feed_dict\n",
    "\n",
    "\n",
    "test_feed_dict = fill_feed_dict('images', 'labels', 'learning_rate', 'keep_prob', 2, 0.01, 1.0)\n",
    "\n",
    "for i in range(len(test_feed_dict['images'])):\n",
    "    plt.figure()\n",
    "    plt.title(str(test_feed_dict['labels'][i]))\n",
    "    plt.imshow(test_feed_dict['images'][i].squeeze(), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "WEIGHT_PENALTY_RATE = 3e-3\n",
    "\n",
    "\n",
    "def weight_variable(shape, stddev):\n",
    "    global WEIGHT_PENALTY_RATE\n",
    "    \n",
    "    initial = tf.truncated_normal(shape, stddev=stddev)\n",
    "    weights = tf.Variable(initial)\n",
    "    \n",
    "    tf.add_to_collection('losses', tf.mul(tf.nn.l2_loss(weights), WEIGHT_PENALTY_RATE))\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def bias_variable(shape, init):\n",
    "    global WEIGHT_PENALTY_RATE\n",
    "    \n",
    "    initial = tf.constant(init, shape=shape)\n",
    "    biases = tf.Variable(initial)\n",
    "    \n",
    "    tf.add_to_collection('losses', tf.mul(tf.nn.l2_loss(biases), WEIGHT_PENALTY_RATE))\n",
    "    \n",
    "    return biases\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                          strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make sure we don't count losses multiple times by resetting the graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, SEGMENT_SIZE, SEGMENT_SIZE, 1], name='input')\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 2])\n",
    "\n",
    "W_conv1 = weight_variable([5, 5, 1, 32], 1e-4)\n",
    "b_conv1 = bias_variable([32], 0.1)\n",
    "h_conv1 = tf.nn.relu(conv2d(x, W_conv1) + b_conv1)\n",
    "\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "W_conv2 = weight_variable([5, 5, 32, 64], 1e-4)\n",
    "b_conv2 = bias_variable([64], 0.1)\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "n = SEGMENT_SIZE // (2 ** 4)\n",
    "W_fc1 = weight_variable([n * n * 64, 512], 0.04)\n",
    "b_fc1 = bias_variable([512], 0.1)\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, n * n * 64])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "# For some reason the Android version of TF doesn't like dropout\n",
    "#pl_keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "#h_fc1_drop = tf.nn.dropout(h_fc1, pl_keep_prob)\n",
    "pl_keep_prob = 'foo'\n",
    "\n",
    "W_fc2 = weight_variable([512, 2], 0.1)\n",
    "b_fc2 = bias_variable([2], 0.1)\n",
    "\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2, name='output')\n",
    "\n",
    "\n",
    "cross_entropy_mean = -tf.reduce_mean(y_ * tf.log(tf.clip_by_value(y_conv, 1e-10, 1)))\n",
    "tf.add_to_collection('losses', cross_entropy_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.add_n(tf.get_collection('losses'))\n",
    "\n",
    "pl_learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "\n",
    "train_step = tf.train.MomentumOptimizer(pl_learning_rate, momentum=0.9).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.arg_max(y_conv, 1), tf.arg_max(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "test_interval = 600\n",
    "test_iters = 300\n",
    "display_interval = 50\n",
    "n_iters = test_interval * 60\n",
    "\n",
    "learning_rate = 0.003\n",
    "step_size = test_interval * 30\n",
    "lr_step_rate = 0.3\n",
    "batch_size = 100\n",
    "\n",
    "all_validation_accuracies = []\n",
    "all_validation_losses = []\n",
    "all_training_losses = []\n",
    "\n",
    "for i in range(n_iters):\n",
    "    if i % test_interval == 0:\n",
    "        validation_accuracies = []\n",
    "        validation_losses = []\n",
    "        \n",
    "        for j in range(test_iters):\n",
    "            feed_dict = fill_feed_dict(x, y_, pl_learning_rate, pl_keep_prob, batch_size, 0, 1)\n",
    "            validation_accuracy, validation_loss = sess.run([accuracy, loss], feed_dict=feed_dict)\n",
    "        \n",
    "            validation_accuracies.append(validation_accuracy)\n",
    "            validation_losses.append(validation_loss)\n",
    "        \n",
    "        mean_validation_accuracy = np.mean(validation_accuracies)\n",
    "        mean_validation_loss = np.mean(validation_losses)\n",
    "        \n",
    "        print('Validation accuracy: ' + str(mean_validation_accuracy))\n",
    "        print('Validation loss: ' + str(mean_validation_loss))\n",
    "        \n",
    "        all_validation_accuracies.append(mean_validation_accuracy)\n",
    "        all_validation_losses.append(mean_validation_loss)\n",
    "        \n",
    "    feed_dict = fill_feed_dict(x, y_, pl_learning_rate, pl_keep_prob, batch_size, learning_rate, 0.5)\n",
    "    _, training_loss = sess.run([train_step, loss], feed_dict=feed_dict)\n",
    "    \n",
    "    if i % display_interval == 0:\n",
    "        print('Training loss: ' + str(training_loss))\n",
    "        print('========')\n",
    "        \n",
    "        all_training_losses.append(training_loss)\n",
    "        \n",
    "    if i % step_size == 0 and i != 0:\n",
    "        learning_rate = learning_rate * lr_step_rate\n",
    "        print('Updated learning rate: ' + str(learning_rate))\n",
    "        print('========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "completed_iters = display_interval * len(all_training_losses)\n",
    "\n",
    "ind_training = range(0, completed_iters, display_interval)\n",
    "plt.plot(ind_training, all_training_losses)\n",
    "\n",
    "ind_validation = range(0, completed_iters, test_interval)\n",
    "plt.plot(ind_validation, all_validation_losses)\n",
    "plt.plot(ind_validation, all_validation_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "completed_iters = display_interval * len(all_training_losses)\n",
    "\n",
    "ind_training = range(0, completed_iters, display_interval)\n",
    "plt.plot(ind_training, all_training_losses)\n",
    "\n",
    "ind_validation = range(0, completed_iters, test_interval)\n",
    "plt.plot(ind_validation, all_validation_losses)\n",
    "plt.plot(ind_validation, all_validation_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and freeze the created graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './saved_checkpoint', 0, 'checkpoint_state')\n",
    "\n",
    "tf.train.write_graph(sess.graph.as_graph_def(), '.', 'input_graph.pb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from freeze_graph import freeze_graph\n",
    "\n",
    "freeze_graph('./input_graph.pb', '', False, './saved_checkpoint-0',\n",
    "             'output', 'save/restore_all',\n",
    "             'save/Const:0', '../second-sight/assets/tensorflow_text_detector.pb',\n",
    "             False, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd ..\n",
    "\n",
    "bazel mobile-install //second-sight:second-sight --start_app"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
