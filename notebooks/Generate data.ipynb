{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the images\n",
    "The [MS COCO](http://mscoco.org/) 2014 dataset has 40,000 images for validation, and 80,000 for training. Text annotations are only available for the training set.\n",
    "\n",
    "Note that the .zip file is relatively large (about 14 GB), so the following cell will take a long time to execute.\n",
    "\n",
    "[arXiv:1405.0312](http://arxiv.org/abs/1405.0312)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir ../data\n",
    "cd ../data\n",
    "\n",
    "wget http://msvocds.blob.core.windows.net/coco2014/train2014.zip\n",
    "unzip train2014.zip\n",
    "rm train2014.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the annotations\n",
    "The [COCO-Text](http://vision.cornell.edu/se3/coco-text/) dataset has 170,000 text instances; about half of the MS COCO images contain text. Annotations are provided for both the training set and the validation set.\n",
    "\n",
    "[arXiv:1601.07140](http://arxiv.org/abs/1405.0312)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd ../data\n",
    "\n",
    "wget https://s3.amazonaws.com/cocotext/COCO_Text.zip\n",
    "unzip COCO_Text.zip\n",
    "rm COCO_Text.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the images into labeled segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The maximum number of training images per class\n",
    "N_TRAIN = 30000\n",
    "# The maximum number of validation images per class\n",
    "N_VALID = 15000\n",
    "# The minimum overlap area between an annotation and a segment\n",
    "# if has_text(...) returns True\n",
    "OVERLAP_THRESHOLD = 500\n",
    "# The size of the sides of the segments, in pixels\n",
    "SEGMENT_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def filename_to_id(filename):\n",
    "    # Extract ID\n",
    "    image_id = filename.split('_')[2].split('.')[0]\n",
    "    # Remove leading zeros\n",
    "    image_id = str(int(image_id))\n",
    "    \n",
    "    return image_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_overlap(rect1, rect2):\n",
    "    \"\"\"Calculate the overlap between two rectangles.\n",
    "    \n",
    "    Assumes that the rectangles are provided as 4-tuples, and the values are as follows:\n",
    "        0. X coordinate of the top-left corner\n",
    "        1. Y coordinate of the top-left corner\n",
    "        2. X coordinate of the bottom-right corner\n",
    "        3. Y coordinate of the bottom-right corner\n",
    "    \"\"\"\n",
    "    \n",
    "    horizontal_overlap = max(0, min(rect1[2], rect2[2]) - max(rect1[0], rect2[0]))\n",
    "    vertical_overlap = max(0, min(rect1[3], rect2[3]) - max(rect1[1], rect2[1]))\n",
    "    \n",
    "    return horizontal_overlap * vertical_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def has_text(image_id, segment_rect, annotation_data):\n",
    "    global OVERLAP_THRESHOLD\n",
    "    \n",
    "    try:\n",
    "        annotation_ids = annotation_data['imgToAnns'][image_id]\n",
    "    except KeyError:\n",
    "        annotation_ids = []\n",
    "    \n",
    "    annotation_ids = [str(annotation_id) for annotation_id in annotation_ids]\n",
    "        \n",
    "    has_text = False\n",
    "    \n",
    "    for annotation_id in annotation_ids:\n",
    "        annotation = annotation_data['anns'][annotation_id]\n",
    "        \n",
    "        legible = annotation['legibility'] == 'legible'\n",
    "        english = annotation['language'] == 'english'\n",
    "        \n",
    "        if legible and english:\n",
    "            bounding_box = annotation['bbox']\n",
    "            \n",
    "            x0, y0 = bounding_box[1], bounding_box[0]\n",
    "            x3, y3 = x0 + bounding_box[3], y0 + bounding_box[2]\n",
    "            \n",
    "            annotation_rect = (x0, y0, x3, y3)\n",
    "            \n",
    "            overlap = calculate_overlap(annotation_rect, segment_rect)\n",
    "            \n",
    "            if overlap > OVERLAP_THRESHOLD:\n",
    "                has_text = True\n",
    "                break\n",
    "                            \n",
    "    return has_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "\n",
    "def get_segments(filename):\n",
    "    \"\"\"Loads the specified file from disk and converts it to multiple segments.\n",
    "    \n",
    "    The segments are single channel images with size (SEGMENT_SIZE x SEGMENT_SIZE).\n",
    "    Padding is done using uniformly distributed random values.\n",
    "    \"\"\"\n",
    "    \n",
    "    global SEGMENT_SIZE\n",
    "    \n",
    "    image = Image.open('../data/train2014/' + filename)\n",
    "    \n",
    "    segments = []\n",
    "    segment_rects = []\n",
    "    \n",
    "    width, height = image.size\n",
    "    \n",
    "    padded_width = int(ceil(width / SEGMENT_SIZE) * SEGMENT_SIZE)\n",
    "    padded_height = int(ceil(height / SEGMENT_SIZE) * SEGMENT_SIZE)\n",
    "    \n",
    "    padded_image = np.random.uniform(0, 256, (padded_height, padded_width))\n",
    "    \n",
    "    # Convert the loaded image to grayscale and add random padding\n",
    "    try:\n",
    "        b, g, r = image.split()\n",
    "        gray_image = np.multiply(0.21, r) + np.multiply(0.72, g) + np.multiply(0.07, b)\n",
    "        padded_image[:height, :width] = gray_image\n",
    "    except ValueError:\n",
    "        padded_image[:height, :width] = image\n",
    "        \n",
    "    for x in range(0, padded_width, SEGMENT_SIZE):\n",
    "        for y in range(0, padded_height, SEGMENT_SIZE):\n",
    "            segment = padded_image[x:x+SEGMENT_SIZE, y:y+SEGMENT_SIZE]\n",
    "            segments.append(segment)\n",
    "            \n",
    "            rect = (x, y, x + SEGMENT_SIZE, y + SEGMENT_SIZE)\n",
    "            segment_rects.append(rect)\n",
    "            \n",
    "    return segments, segment_rects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.misc import imsave\n",
    "\n",
    "def save_image(path, image):\n",
    "    \"\"\"Tries to save a given image to a given path.\n",
    "    \n",
    "    Returns: the number of images successfully saved. (Either 0 or 1.)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        imsave(path, image)\n",
    "    except ValueError:\n",
    "        return 0\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd ../data\n",
    "\n",
    "mkdir train\n",
    "mkdir train/text\n",
    "mkdir train/no-text\n",
    "\n",
    "mkdir valid\n",
    "mkdir valid/text\n",
    "mkdir valid/no-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 82783 images.\n",
      "Text annotations loaded.\n"
     ]
    }
   ],
   "source": [
    "from os import walk\n",
    "import json\n",
    "\n",
    "images = next(walk('../data/train2014'))[2]\n",
    "print('The dataset has ' + str(len(images)) + ' images.')\n",
    "\n",
    "annotation_data = json.load(open('../data/COCO_Text.json'))\n",
    "print('Text annotations loaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.shuffle(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed: 56631 / 60000\n"
     ]
    }
   ],
   "source": [
    "n_text = 0\n",
    "n_no_text = 0\n",
    "\n",
    "i_last_training_image = 0\n",
    "\n",
    "for filename in images:\n",
    "    if n_text == N_TRAIN and n_no_text == N_TRAIN:\n",
    "        break\n",
    "    \n",
    "    i_last_training_image += 1\n",
    "    image_id = filename_to_id(filename)\n",
    "    \n",
    "    segments, segment_rects = get_segments(filename)\n",
    "    \n",
    "    for i in range(len(segments)):\n",
    "        if has_text(image_id, segment_rects[i], annotation_data) and n_text < N_TRAIN:\n",
    "            n_text += save_image('../data/train/text/' + str(n_text) + '.jpg', segments[i])\n",
    "        elif n_no_text < N_TRAIN:\n",
    "            n_no_text += save_image('../data/train/no-text/' + str(n_no_text) + '.jpg', segments[i])\n",
    "            \n",
    "print('Successfully processed: ' + str(n_text + n_no_text) + ' / ' + str(N_TRAIN * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed: 0 / 30000\n"
     ]
    }
   ],
   "source": [
    "n_text = 0\n",
    "n_no_text = 0\n",
    "\n",
    "for filename in images[i_last_training_image:]:\n",
    "    if n_text == N_VALID and n_no_text == N_VALID:\n",
    "        break\n",
    "    \n",
    "    image_id = filename_to_id(filename)\n",
    "    \n",
    "    segments, segment_rects = get_segments(filename)\n",
    "    \n",
    "    for i in range(len(segments)):\n",
    "        if has_text(image_id, segment_rects[i], annotation_data) and n_text < N_VALID:\n",
    "            n_text += save_image('../data/valid/text/' + str(n_text) + '.jpg', segments[i])\n",
    "        elif n_no_text < N_VALID:\n",
    "            n_no_text += save_image('../data/valid/no-text/' + str(n_no_text) + '.jpg', segments[i])\n",
    "            \n",
    "print('Successfully processed: ' + str(n_text + n_no_text) + ' / ' + str(N_VALID * 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPython (Python 3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
